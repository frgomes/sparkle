sparkle {
  dump-config = ["/tmp/kafka-tests.conf.out"]

  logging {
    // zookeeper and kafka are hard coded to use log4j
    provider = log4j
    
    levels {
      root = DEBUG
      "kafka.network" = WARN
      "kafka.utils" = WARN
      "kafka.client" = WARN
      "kafka.consumer" = ERROR
      "kafka.consumer.SimpleConsumer" = ERROR
      "kafka.producer.SyncProducer" = WARN
      "kafka.producer.BrokerPartitionInfo" = ERROR
      "kafka.producer.async.DefaultEventHandler" = FATAL
      "com.datastax" = WARN
      "org.apache.zookeeper.ClientCnxn" = ERROR
      "com.datastax.driver.core.Cluster" = ERROR  // ignore warnings as test dbs are shut down
      "com.datastax.driver.core.Metadata" = FAIL  // ignore warnings as test dbs are shut down
    }
    
    file {
      path = "/tmp/sparkle-kafka-tests.log"
      level = TRACE
    }
        
  }
  
  measure {
    metrics-gateway {
      enable = false
    }
    
    tsv-gateway {
      enable = true
      file = "/tmp/kafka-loader-measurements.tsv"
    }
  }
  
  kafka-loader {
    //message-commit-limit = 10
    //max-concurrent-writes = 200
    
    reader {
      // the kafka reader uses a consumer group per topic to avoid issues restarting the
      consumer-group-prefix = "itConsumer"
      consumer-group-prefix = ${?SPARKLE_CONSUMER_GROUP_PREFIX}
    }
  }

}
