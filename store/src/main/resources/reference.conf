sparkle {

  // driver to use for reading (storage is typically e.g. cassandra or RAM.  Or build your own that fulfills the Storage api)
  store = nest.sparkle.store.cassandra.ConfiguredCassandraReader

  // driver to use for writing to storage (e.g. for stream loading)
  writeable-store = nest.sparkle.store.cassandra.ConfiguredCassandraWriter

  // driver to use for writing to storage (e.g. for stream loading)
  read-write-store = nest.sparkle.store.cassandra.ConfiguredCassandraReaderWriter

  // erase and reformat the store on launch (e.g. for integration tests)
  erase-store = false

  // Must be thread safe, extend nest.sparkle.store.ColumnPathFormat, and have a parameterless
  // constructor.
  // For determining the column category of a column path, where column category refers to
  // what kind of column the column path represents, without taking into account any user,
  // device, etc specific info. E.g. the column category for column paths like
  // "server1/responseLatency/p99", "server2/responseLatency/p99", etc could be
  // "responseLatency/p99". The default, nest.sparkle.store.BasicColumnPathFormat, just
  // returns the column path as the column category.
  column-path-format = "nest.sparkle.store.BasicColumnPathFormat"
  
  // configuration for the cassandra storage
  sparkle-store-cassandra {
    // one or more of the cassandra server hosts.  
    contact-hosts = [localhost]

    // list of all cassandra data centers, used when creating the keyspace to configure
    // NetworkTopologyStrategy replication strategy 
    data-centers = [datacenter1]
    
    // All sparkle data goes into a keyspace with this name.
    // The keyspace is created on demand if it does not exist. 
    // Multiple servers can share the same cassandra service if they use separate keyspaces.
    key-space = events
    
    // replication factor for the sparkle cassandra keyspace
    // note that this only takes effect when the keyspace is first created. 
    replication-factor = 1

    // consistency level for reads
    // value is a string from com.datastax.driver.core.ConsistencyLevel
    read-consistency-level = LOCAL_ONE

    // consistency level for writes
    // value is a string from com.datastax.driver.core.ConsistencyLevel
    write-consistency-level = LOCAL_ONE

    // max size of LRU cache used to track column categories that this instance has read/written
    // from/to cassandra, so we can avoid duplicate reads/writes
    column-categories-max-cache-size = 1000

    // max size of LRU cache used to track entities that this instance has written
    // to cassandra, so we can avoid duplicate writes
    entity-catalog-max-cache-size = 100000

    // max number of statements per batch write request to cassandra (this equates to
    // how many events are written to cassandra in a single request), see Cassandra-6487 where
    // they recommend a batchSize of ~100
    write-batch-size = 100

    // true to store an internal catalog of columnPath path elements
    dataset-catalog-enabled = false
  }

files-loader {
    // true to start the files loader when launching
    auto-start = false

    // set to paths of file system directories containing data files to load into the server
    // the directories are recursively searched for .csv/.tsv files
    directories = []

    // set to true to continuously watch the directories listed
    watch-directories = true

    // load .csv/.tsv lines in batches up to this size
    // (larger batches use more memory but might be faster)
    batch-size = 10000
  }
}
